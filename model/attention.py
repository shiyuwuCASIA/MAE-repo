# -*- coding: utf-8 -*-
""" Attention module in Transformer. 
    Linear module for generating Q, K, V is also added here. 
    
    All data here are torch.tensor or torch.cuda.tensor. 
"""

import torch
import torch.nn as nn

class ScaledDotProductAttention(nn.Module): 
    """ Compute Scaled Dot-Product Attention. 
        Mask is optional for this module. 
    """

    def __init__(self, d_k=64, attn_drop=0.): 
        """ This class is used by MultiHeadAttention, where d_k is a constant. 
            
            d_k: dimension of queries and keys
        """

        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k
        self.attn_drop = nn.Dropout(p=attn_drop) if attn_drop > 0 else nn.Identity()
    
    def forward(self, query, key, value, mask=None): 
        """ 
            softmax([batch, head, len, d_k] * [batch, head, d_k, len] / sqrt(d_k)) x [batch, head, len, d_v]
        """

        #when training with mixed-precision, there may be some errors on matmul
        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_k ** 0.5)

        if mask is not None: 
            """ Here mask is used to remove the influence of sentences padding and subsequent words. 
                The mask is generated by the framework, and subsequent mask is also added there. 
                Within a mask, use True(or 1) to represent padding. 
            """
            attention_scores = attention_scores.masked_fill_(mask, -1e9)
        
        attention_probs = attention_scores.softmax(dim=-1)#by row

        #dropout can be added to attn here
        attention_probs = self.attn_drop(attention_probs)

        context = torch.matmul(attention_probs, value)

        return context
    


class MultiHeadAttention(nn.Module): 
    def __init__(self, d_model=512, n_heads=8, d_k=64, d_v=64, attn_drop=0., proj_drop=0., qkv_bias=False): 
        super(MultiHeadAttention, self).__init__()
        """ (Masked) Multi-Head Attention. 
            Here d_k, d_v and n_heads are constants. 
            Function as one sub-layer. 

            Dropout is at the end of this module. 
        """

        self.d_k = d_k#dimension of queries and keys
        self.d_v = d_v#dimension of values
        self.n_heads = n_heads#multi-head

        self.attention = ScaledDotProductAttention(self.d_k, attn_drop=attn_drop)

        #linear input and output are euqal in most cases
        self.W_Q = nn.Linear(d_model, self.d_k * self.n_heads, bias=qkv_bias)
        self.W_K = nn.Linear(d_model, self.d_k * self.n_heads, bias=qkv_bias)
        self.W_V = nn.Linear(d_model, self.d_v * self.n_heads, bias=qkv_bias)
        
        self.proj = nn.Linear(self.n_heads * self.d_v, d_model)
        self.proj_drop = nn.Dropout(p=proj_drop) if proj_drop > 0 else nn.Identity()
    
    def forward(self, input_Q, input_K, input_V, mask=None): 
        """ Intput Q, K, V are origin data generated by other layers. 
            They will be decoded by this module. 
        """

        batch_size = input_Q.size(0)#assert all input parameters have same batch size

        if mask is not None: 
            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)
        
        #generate Q, K, V
        #(batch, n_heads, max_len, d_k)
        query = self.W_Q(input_Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        key = self.W_K(input_K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        value = self.W_V(input_V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)

        context = self.attention(query, key, value, mask)
        context = context.transpose(1, 2).reshape(batch_size, -1, self.n_heads * self.d_v)#be contiguous
        
        #outputs size is same to intput_V
        #(batch, len, d_model)
        outputs = self.proj(context)
        #dropout
        outputs = self.proj_drop(outputs)
        
        return outputs
